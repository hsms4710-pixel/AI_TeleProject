#!/usr/bin/env python3
"""
CSIBERT æ¨¡å‹è®­ç»ƒè„šæœ¬ / CSIBERT Model Training Script

ä¸»è¦åŠŸèƒ½ / Main Features:
- åŠ è½½å’Œé¢„å¤„ç† CSI æ•°æ® / Load and preprocess CSI data
- æ•°æ®æ‹†åˆ†ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼‰/ Data split (train/validation/test)
- æ¨¡å‹è®­ç»ƒä¸éªŒè¯ / Model training and validation
- ä¿å­˜æœ€ä½³æ¨¡å‹ / Save best model
- ç”Ÿæˆè®­ç»ƒæ›²çº¿ / Generate training curves

ä½¿ç”¨æ–¹æ³• / Usage:
    python train.py --hidden_size 256 --num_epochs 50 --batch_size 16
"""

import os
import argparse
import numpy as np
import scipy.io
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW
from transformers import get_scheduler
from sklearn.model_selection import train_test_split

from model import CSIBERT


# æ£€æµ‹å¯ç”¨è®¾å¤‡ / Detect available device
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"ä½¿ç”¨ CUDA GPU: {torch.cuda.get_device_name(0)}")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print("ä½¿ç”¨ Apple MPS")
else:
    device = torch.device("cpu")
    print("ä½¿ç”¨ CPU")


def load_csi_data(data_path='foundation_model_data/csi_data_massive_mimo.mat'):
    """
    åŠ è½½ CSI æ•°æ® / Load CSI data
    
    Args:
        data_path: .mat æ–‡ä»¶è·¯å¾„ / Path to .mat file
        
    Returns:
        cell_data: åŸå§‹ CSI æ•°æ® / Raw CSI data
    """
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
    cell_data = scipy.io.loadmat(data_path)['multi_cell_csi']
    print(f"   æ•°æ®å½¢çŠ¶: {cell_data.shape}")
    return cell_data


def preprocess_csi_matrix(csi_matrix):
    """
    é¢„å¤„ç†å•ä¸ª CSI çŸ©é˜µ / Preprocess a single CSI matrix
    
    å¤„ç†æ­¥éª¤ / Processing steps:
    1. åˆ†ç¦»å®éƒ¨å’Œè™šéƒ¨ / Separate real and imaginary parts
    2. æ ‡å‡†åŒ– / Normalize
    3. å±•å¹³ä¸ºç‰¹å¾å‘é‡ / Flatten to feature vector
    
    Args:
        csi_matrix: å¤æ•° CSI çŸ©é˜µ / Complex CSI matrix
        
    Returns:
        csi_combined: é¢„å¤„ç†åçš„ CSI (time, feature_dim)
    """
    # åˆ†ç¦»å®éƒ¨å’Œè™šéƒ¨ / Separate real and imaginary parts
    csi_real = np.real(csi_matrix)
    csi_imag = np.imag(csi_matrix)
    
    # æ ‡å‡†åŒ– / Normalize
    csi_real_normalized = (csi_real - np.mean(csi_real)) / (np.std(csi_real) + 1e-8)
    csi_imag_normalized = (csi_imag - np.mean(csi_imag)) / (np.std(csi_imag) + 1e-8)

    # ç»„åˆå¹¶å±•å¹³ / Combine and flatten
    csi_combined = np.stack([csi_real_normalized, csi_imag_normalized], axis=-1)
    time_dim = csi_combined.shape[0]
    feature_dim = np.prod(csi_combined.shape[1:])
    csi_combined = csi_combined.reshape(time_dim, feature_dim)
    
    return csi_combined


def mask_data(data, mask_ratio=0.15):
    """
    å¯¹æ•°æ®åº”ç”¨æ©ç ï¼ˆç”¨äºè‡ªç›‘ç£å­¦ä¹ ï¼‰/ Apply masking to data (for self-supervised learning)
    
    Args:
        data: è¾“å…¥æ•°æ® / Input data
        mask_ratio: æ©ç æ¯”ä¾‹ / Mask ratio
        
    Returns:
        masked_data: æ©ç åçš„æ•°æ® / Masked data
        mask: æ©ç ä½ç½® / Mask positions
    """
    mask = np.random.rand(*data.shape) < mask_ratio
    masked_data = np.copy(data)
    masked_data[mask] = 0
    return masked_data, mask


def create_dataloader(data, batch_size, shuffle=True):
    """
    åˆ›å»º DataLoader / Create DataLoader
    
    å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œæ©ç å¤„ç†ï¼Œå¹¶åˆ›å»º DataLoader
    
    Args:
        data: æ•°æ®åˆ—è¡¨ / List of data samples
        batch_size: æ‰¹æ¬¡å¤§å° / Batch size
        shuffle: æ˜¯å¦æ‰“ä¹± / Whether to shuffle
        
    Returns:
        DataLoader å¯¹è±¡
    """
    # å¯¹æ•°æ®è¿›è¡Œæ©ç  / Apply masking
    masked_data, masks = zip(*[mask_data(d) for d in data])
    
    # å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦ / Pad sequences to same length
    max_len = max(len(d) for d in data)
    feature_dim = data[0].shape[1]
    
    padded_inputs = np.zeros((len(data), max_len, feature_dim), dtype=np.float32)
    padded_labels = np.zeros((len(data), max_len, feature_dim), dtype=np.float32)
    attention_masks = np.zeros((len(data), max_len), dtype=np.float32)
    
    for i, (masked, original) in enumerate(zip(masked_data, data)):
        seq_len = len(original)
        padded_inputs[i, :seq_len, :] = masked
        padded_labels[i, :seq_len, :] = original
        attention_masks[i, :seq_len] = 1
    
    # è½¬æ¢ä¸º PyTorch å¼ é‡ / Convert to PyTorch tensors
    inputs_tensor = torch.from_numpy(padded_inputs).float()
    labels_tensor = torch.from_numpy(padded_labels).float()
    masks_tensor = torch.from_numpy(attention_masks).float()
    
    dataset = TensorDataset(inputs_tensor, labels_tensor, masks_tensor)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


def main(hidden_size, num_hidden_layers, num_attention_heads, learning_rate, batch_size, num_epochs, patience):
    """
    ä¸»è®­ç»ƒå‡½æ•° / Main training function
    
    Args:
        patience: æ—©åœè€å¿ƒå€¼ï¼ŒéªŒè¯æŸå¤±è¿ç»­å¤šå°‘è½®ä¸æ”¹å–„åˆ™åœæ­¢è®­ç»ƒã€‚è®¾ä¸º0åˆ™ç¦ç”¨æ—©åœã€‚
    """
    print("\n" + "="*70)
    print("ğŸš€ CSIBERT è®­ç»ƒæµç¨‹å¼€å§‹ / CSIBERT Training Pipeline Started")
    print("="*70)
    
    # 1ï¸âƒ£ åŠ è½½å¹¶é¢„å¤„ç†æ•°æ® / Load and preprocess data
    cell_data = load_csi_data()
    
    preprocessed_data = []
    print("ğŸ”„ é¢„å¤„ç†æ•°æ®ä¸­...")
    
    for cell_idx in range(cell_data.shape[0]):
        for ue_idx in range(cell_data.shape[1]):
            ue_data = cell_data[cell_idx, ue_idx]
            for scenario in ue_data[0]:
                processed_csi = preprocess_csi_matrix(scenario)
                preprocessed_data.append(processed_csi)
    
    print(f"   âœ“ é¢„å¤„ç†å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(preprocessed_data)}")
    
    # 2ï¸âƒ£ æ•°æ®æ‹†åˆ† / Data split
    print("\nğŸ“Š æ•°æ®æ‹†åˆ†:")
    # å…ˆåˆ†å‡ºæµ‹è¯•é›† (20%)
    train_val_data, test_data = train_test_split(
        preprocessed_data, test_size=0.2, random_state=42
    )
    # å†ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‡ºéªŒè¯é›† (10% of total = 12.5% of train_val)
    train_data, val_data = train_test_split(
        train_val_data, test_size=0.125, random_state=42
    )
    
    print(f"   è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬ (70%)")
    print(f"   éªŒè¯é›†: {len(val_data)} æ ·æœ¬ (10%)")
    print(f"   æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬ (20%)")
    
    # ä¿å­˜æµ‹è¯•é›†ä¾›åç»­éªŒè¯ä½¿ç”¨ / Save test set for later validation
    os.makedirs('validation_data', exist_ok=True)
    np.save('validation_data/test_data.npy', np.array(test_data, dtype=object))
    print(f"   âœ“ æµ‹è¯•é›†å·²ä¿å­˜è‡³ validation_data/test_data.npy")
    
    # 3ï¸âƒ£ åˆ›å»º DataLoader / Create DataLoaders
    print("\nğŸ”§ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader = create_dataloader(train_data, batch_size, shuffle=True)
    val_loader = create_dataloader(val_data, batch_size, shuffle=False)
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # 4ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹ / Initialize model
    feature_dim = preprocessed_data[0].shape[1]
    print(f"\nğŸ§  åˆå§‹åŒ–æ¨¡å‹:")
    print(f"   ç‰¹å¾ç»´åº¦: {feature_dim}")
    print(f"   éšè—å±‚å¤§å°: {hidden_size}")
    print(f"   Transformer å±‚æ•°: {num_hidden_layers}")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: {num_attention_heads}")
    
    model = CSIBERT(
        feature_dim=feature_dim,
        hidden_size=hidden_size,
        num_hidden_layers=num_hidden_layers,
        num_attention_heads=num_attention_heads
    ).to(device)
    
    # 5ï¸âƒ£ åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ / Initialize optimizer and scheduler
    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    num_training_steps = num_epochs * len(train_loader)
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=num_training_steps // 10,  # 10% warmup
        num_training_steps=num_training_steps
    )
    
    loss_function = nn.MSELoss()
    
    # 6ï¸âƒ£ è®­ç»ƒå¾ªç¯ / Training loop
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ:")
    print(f"   æœ€å¤§è½®æ•°: {num_epochs}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    if patience > 0:
        print(f"   æ—©åœè€å¿ƒå€¼: {patience}")
    else:
        print(f"   æ—©åœ: ç¦ç”¨")
    print(f"   è®¾å¤‡: {device}")
    print("="*70 + "\n")
    
    best_val_loss = float('inf')
    best_epoch = 0
    patience_counter = 0
    train_losses = []
    val_losses = []
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ / Training phase
        model.train()
        total_train_loss = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [è®­ç»ƒ]")
        for batch in progress_bar:
            inputs, labels, attention_mask = [b.to(device) for b in batch]
            
            # å‰å‘ä¼ æ’­ / Forward pass
            outputs = model(inputs, attention_mask=attention_mask)
            loss = loss_function(outputs, labels)
            
            # åå‘ä¼ æ’­ / Backward pass
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            
            total_train_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.6f}'})
        
        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ / Validation phase
        model.eval()
        total_val_loss = 0
        
        with torch.no_grad():
            for batch in val_loader:
                inputs, labels, attention_mask = [b.to(device) for b in batch]
                outputs = model(inputs, attention_mask=attention_mask)
                loss = loss_function(outputs, labels)
                total_val_loss += loss.item()
        
        avg_val_loss = total_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        # æ‰“å°å½“å‰è½®æ¬¡ç»“æœ / Print current epoch results
        print(f"Epoch {epoch+1:3d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}", end="")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ / Save best model
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_epoch = epoch + 1
            patience_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': best_val_loss,
                'train_loss': avg_train_loss,
                'feature_dim': feature_dim,
                'hidden_size': hidden_size,
                'num_hidden_layers': num_hidden_layers,
                'num_attention_heads': num_attention_heads
            }
            os.makedirs('checkpoints', exist_ok=True)
            torch.save(checkpoint, 'checkpoints/best_model.pt')
            print(" âœ“ [å·²ä¿å­˜æœ€ä½³æ¨¡å‹]")
        else:
            patience_counter += 1
            if patience > 0:
                print(f" (æœªæ”¹å–„: {patience_counter}/{patience})")
            else:
                print()
        
        # æ—©åœæ£€æŸ¥ / Early stopping check
        if patience > 0 and patience_counter >= patience:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±è¿ç»­ {patience} è½®æœªæ”¹å–„")
            print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch})")
            print(f"   è®­ç»ƒåœ¨ç¬¬ {epoch+1} è½®åœæ­¢")
            break
    
    # 7ï¸âƒ£ ç»˜åˆ¶è®­ç»ƒæ›²çº¿ / Plot training curves
    print("\nğŸ“ˆ ç”Ÿæˆè®­ç»ƒæ›²çº¿...")
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Training Loss', linewidth=2)
    plt.plot(val_losses, label='Validation Loss', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_losses, label='Training Loss', linewidth=2)
    plt.plot(val_losses, label='Validation Loss', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss (log scale)')
    plt.yscale('log')
    plt.title('Training and Validation Loss (Log Scale)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_validation_loss.png', dpi=300, bbox_inches='tight')
    print(f"   âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ training_validation_loss.png")
    
    # 8ï¸âƒ£ ä¿å­˜æŸå¤±å†å² / Save loss history
    np.savez('training_history.npz', 
             train_losses=train_losses, 
             val_losses=val_losses,
             best_val_loss=best_val_loss)
    print(f"   âœ“ è®­ç»ƒå†å²å·²ä¿å­˜è‡³ training_history.npz")
    
    print("\n" + "="*70)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch})")
    print(f"   å®é™…è®­ç»ƒè½®æ•°: {len(train_losses)}/{num_epochs}")
    print("="*70)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='è®­ç»ƒ CSIBERT æ¨¡å‹ / Train CSIBERT Model')
    
    # æ¨¡å‹å‚æ•° / Model parameters
    parser.add_argument('--hidden_size', type=int, default=256,
                        help='éšè—å±‚å¤§å° / Hidden layer size (default: 256)')
    parser.add_argument('--num_hidden_layers', type=int, default=4,
                        help='Transformer å±‚æ•° / Number of Transformer layers (default: 4)')
    parser.add_argument('--num_attention_heads', type=int, default=4,
                        help='æ³¨æ„åŠ›å¤´æ•° / Number of attention heads (default: 4)')
    
    # è®­ç»ƒå‚æ•° / Training parameters
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                        help='å­¦ä¹ ç‡ / Learning rate (default: 1e-4)')
    parser.add_argument('--batch_size', type=int, default=16,
                        help='æ‰¹æ¬¡å¤§å° / Batch size (default: 16)')
    parser.add_argument('--num_epochs', type=int, default=50,
                        help='è®­ç»ƒè½®æ•° / Number of epochs (default: 50)')
    parser.add_argument('--patience', type=int, default=15,
                        help='æ—©åœè€å¿ƒå€¼ / Early stopping patience (default: 15, 0=disable)')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œè®­ç»ƒ / Execute training
    main(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        num_attention_heads=args.num_attention_heads,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        num_epochs=args.num_epochs,
        patience=args.patience
    )

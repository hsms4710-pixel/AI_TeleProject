#!/usr/bin/env python3
"""
CSIBERT æ¨¡å‹æ€§èƒ½éªŒè¯è„šæœ¬

å®Œæ•´çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼š
1. é‡æ„è¯¯å·® (MSE, NMSE, MAE)
2. é¢„æµ‹å‡†ç¡®åº¦ (æ—¶åºé¢„æµ‹èƒ½åŠ›)
3. ä¿¡å™ªæ¯”åˆ†æ (ä¸åŒSNRä¸‹çš„æ€§èƒ½)
4. é¢‘è°±æ•ˆç‡æå‡
5. å‹ç¼©ç‡ä¸è´¨é‡æƒè¡¡
6. æ³›åŒ–èƒ½åŠ›æµ‹è¯•
7. è®¡ç®—å¤æ‚åº¦åˆ†æ
"""

import torch
import numpy as np
import scipy.io
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
from model import CSIBERT
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tqdm import tqdm
import time
import os
import json

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-darkgrid')
import warnings
warnings.filterwarnings('ignore')


class CSIBERTValidator:
    """CSIBERT æ¨¡å‹éªŒè¯å™¨"""
    
    def __init__(self, model_path, data_path=None, device=None):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            data_path: CSIæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„æµ‹è¯•é›† validation_data/test_data.npyï¼‰
            device: è®¡ç®—è®¾å¤‡ (cuda/cpu)
        """
        self.model_path = model_path
        
        # è®¾ç½®æ•°æ®è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„æµ‹è¯•é›†
        if data_path is None:
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            model_dir = os.path.dirname(os.path.abspath(__file__))
            # ä¼˜å…ˆä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„æµ‹è¯•é›†ï¼ˆç¡®ä¿æ•°æ®æœªå‚ä¸è®­ç»ƒï¼‰
            test_data_path = os.path.join(model_dir, "validation_data", "test_data.npy")
            if os.path.exists(test_data_path):
                self.data_path = test_data_path
                self.use_saved_test_set = True
                print("ğŸ“Š ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„æµ‹è¯•é›†ï¼ˆæœªå‚ä¸è®­ç»ƒçš„æ•°æ®ï¼‰")
            else:
                # å¦‚æœæµ‹è¯•é›†ä¸å­˜åœ¨ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®
                self.data_path = os.path.join(model_dir, "foundation_model_data", "csi_data_massive_mimo.mat")
                self.use_saved_test_set = False
                print("âš ï¸  æœªæ‰¾åˆ°ä¿å­˜çš„æµ‹è¯•é›†ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆå¯èƒ½åŒ…å«è®­ç»ƒæ•°æ®ï¼‰")
        else:
            self.data_path = data_path
            self.use_saved_test_set = data_path.endswith('.npy')
        
        # è®¾ç½®éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
        np.random.seed(42)
        torch.manual_seed(42)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(42)
        
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if device is None:
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
            elif torch.backends.mps.is_available():
                self.device = torch.device("mps")
            else:
                self.device = torch.device("cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ•°æ®è·¯å¾„: {self.data_path}")
        
        # åˆ›å»ºç»“æœè¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿å§‹ç»ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰
        self.project_root = os.path.dirname(os.path.abspath(__file__))
        self.results_dir = os.path.join(self.project_root, 'validation_results')
        os.makedirs(self.results_dir, exist_ok=True)
        
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        self.model = self._load_model()
        self.test_data, self.attention_masks = self._load_and_preprocess_data()
        
        # ç»“æœå­˜å‚¨
        self.results = {}
        
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"\n{'='*60}")
        print("åŠ è½½æ¨¡å‹...")
        print(f"{'='*60}")
        
        checkpoint = torch.load(self.model_path, map_location=self.device)
        
        # æå–æ¨¡å‹é…ç½®
        feature_dim = checkpoint.get("feature_dim")
        hidden_size = checkpoint.get("hidden_size", 256)
        num_hidden_layers = checkpoint.get("num_hidden_layers", 4)
        num_attention_heads = checkpoint.get("num_attention_heads", 4)
        
        print(f"æ¨¡å‹é…ç½®:")
        print(f"  - Feature Dimension: {feature_dim}")
        print(f"  - Hidden Size: {hidden_size}")
        print(f"  - Transformer Layers: {num_hidden_layers}")
        print(f"  - Attention Heads: {num_attention_heads}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = CSIBERT(
            feature_dim=feature_dim,
            hidden_size=hidden_size,
            num_hidden_layers=num_hidden_layers,
            num_attention_heads=num_attention_heads
        )
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        # è®¡ç®—æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"  - æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  - æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")
        
        return model
    
    def _preprocess_csi_matrix(self, csi_matrix):
        """é¢„å¤„ç†å•ä¸ªCSIçŸ©é˜µ"""
        csi_real = np.real(csi_matrix)
        csi_imag = np.imag(csi_matrix)
        
        csi_real_normalized = (csi_real - np.mean(csi_real)) / (np.std(csi_real) + 1e-8)
        csi_imag_normalized = (csi_imag - np.mean(csi_imag)) / (np.std(csi_imag) + 1e-8)
        
        csi_combined = np.stack([csi_real_normalized, csi_imag_normalized], axis=-1)
        time_dim = csi_combined.shape[0]
        feature_dim = np.prod(csi_combined.shape[1:])
        
        return csi_combined.reshape(time_dim, feature_dim)
    
    def _load_and_preprocess_data(self):
        """åŠ è½½å¹¶é¢„å¤„ç†CSIæ•°æ®"""
        print(f"\n{'='*60}")
        print("åŠ è½½æ•°æ®...")
        print(f"{'='*60}")
        
        preprocessed_data = []
        sequence_lengths = []
        
        # åˆ¤æ–­æ•°æ®æºç±»å‹
        if self.use_saved_test_set:
            # åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„æµ‹è¯•é›†ï¼ˆ.npy æ ¼å¼ï¼‰
            print(f"ä»æµ‹è¯•é›†åŠ è½½: {self.data_path}")
            test_data = np.load(self.data_path, allow_pickle=True)
            
            # test_data å·²ç»æ˜¯é¢„å¤„ç†åçš„åˆ—è¡¨
            if isinstance(test_data, np.ndarray) and test_data.dtype == object:
                preprocessed_data = list(test_data)
            else:
                preprocessed_data = [test_data[i] for i in range(len(test_data))]
            
            sequence_lengths = [seq.shape[0] for seq in preprocessed_data]
            print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(preprocessed_data)}")
            
        else:
            # ä»åŸå§‹ MATLAB æ–‡ä»¶åŠ è½½
            print(f"ä» MATLAB æ–‡ä»¶åŠ è½½: {self.data_path}")
            mat_data = scipy.io.loadmat(self.data_path)
            
            # å°è¯•ä¸åŒçš„æ•°æ®é”®
            if 'multi_cell_csi' in mat_data:
                cell_data = mat_data['multi_cell_csi']
            elif 'CSI_data' in mat_data:
                cell_data = mat_data['CSI_data']
            else:
                # æ‰“å°æ‰€æœ‰å¯ç”¨çš„é”®
                available_keys = [k for k in mat_data.keys() if not k.startswith('__')]
                raise KeyError(f"æ‰¾ä¸åˆ° CSI æ•°æ®ã€‚å¯ç”¨çš„é”®: {available_keys}")
            
            print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {cell_data.shape}")
            
            # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„
            if cell_data.ndim == 3:
                # ç®€å•çš„ 3D æ•°ç»„: (samples, time_steps, features)
                print(f"æ£€æµ‹åˆ°ç®€å• 3D æ•°ç»„ç»“æ„")
                num_samples = min(cell_data.shape[0], 1000)  # é™åˆ¶æ ·æœ¬æ•°é‡
                for i in range(num_samples):
                    sample = cell_data[i]
                    processed_csi = self._preprocess_csi_matrix(sample)
                    preprocessed_data.append(processed_csi)
                    sequence_lengths.append(processed_csi.shape[0])
            else:
                # å¤æ‚çš„åµŒå¥—ç»“æ„
                print(f"æ£€æµ‹åˆ°åµŒå¥—æ•°ç»„ç»“æ„")
                # éå†æ‰€æœ‰å°åŒºå’Œç”¨æˆ·
                for cell_idx in range(min(cell_data.shape[0], 5)):  # é™åˆ¶å°åŒºæ•°
                    for ue_idx in range(min(cell_data.shape[1], 20)):  # é™åˆ¶ç”¨æˆ·æ•°
                        ue_data = cell_data[cell_idx, ue_idx]
                        if isinstance(ue_data, np.ndarray) and ue_data.size > 0:
                            # å°è¯•æå–åœºæ™¯æ•°æ®
                            try:
                                for scenario in ue_data[0]:
                                    processed_csi = self._preprocess_csi_matrix(scenario)
                                    preprocessed_data.append(processed_csi)
                                    sequence_lengths.append(processed_csi.shape[0])
                            except:
                                # å¦‚æœæå–å¤±è´¥ï¼Œç›´æ¥å¤„ç†
                                processed_csi = self._preprocess_csi_matrix(ue_data)
                                preprocessed_data.append(processed_csi)
                                sequence_lengths.append(processed_csi.shape[0])
        
        print(f"æ ·æœ¬æ€»æ•°: {len(preprocessed_data)}")
        print(f"å¹³å‡åºåˆ—é•¿åº¦: {np.mean(sequence_lengths):.1f}")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦: {max(sequence_lengths)}")
        print(f"æœ€å°åºåˆ—é•¿åº¦: {min(sequence_lengths)}")
        
        # Paddingå¤„ç†
        max_sequence_length = max(sequence_lengths)
        feature_dim = preprocessed_data[0].shape[-1]
        
        padded_data = np.zeros((len(preprocessed_data), max_sequence_length, feature_dim), 
                               dtype=np.float32)
        attention_masks = np.zeros((len(preprocessed_data), max_sequence_length), 
                                   dtype=np.float32)
        
        for i, sequence in enumerate(preprocessed_data):
            seq_len = sequence.shape[0]
            padded_data[i, :seq_len, :] = sequence
            attention_masks[i, :seq_len] = 1
        
        print(f"å¡«å……åæ•°æ®å½¢çŠ¶: {padded_data.shape}")
        
        return padded_data, attention_masks
    
    def test_reconstruction_error(self, mask_ratio=0.15):
        """
        æµ‹è¯•1: é‡æ„è¯¯å·®
        
        æµ‹è¯•æ¨¡å‹æ¢å¤è¢«mask CSIçš„èƒ½åŠ›
        """
        print(f"\n{'='*60}")
        print("æµ‹è¯• 1: é‡æ„è¯¯å·®åˆ†æ")
        print(f"{'='*60}")
        
        # åˆ›å»ºmaskæ•°æ®
        masked_data = np.copy(self.test_data)
        mask_indices = np.random.rand(*masked_data.shape[:2]) < mask_ratio
        masked_data[mask_indices] = 0
        
        test_dataset = TensorDataset(
            torch.tensor(masked_data).float(),
            torch.tensor(self.test_data).float(),
            torch.tensor(self.attention_masks).float()
        )
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # è¯„ä¼°
        mse_list = []
        nmse_list = []
        mae_list = []
        
        with torch.no_grad():
            for inputs, labels, masks in tqdm(test_loader, desc="é‡æ„æµ‹è¯•"):
                inputs = inputs.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(inputs)
                
                # åªè®¡ç®—æœ‰æ•ˆæ•°æ®çš„è¯¯å·®
                for i in range(len(inputs)):
                    valid_mask = masks[i] == 1
                    valid_len = valid_mask.sum().item()
                    
                    if valid_len > 0:
                        pred = outputs[i, :int(valid_len)].cpu().numpy()
                        true = labels[i, :int(valid_len)].cpu().numpy()
                        
                        mse = mean_squared_error(true.flatten(), pred.flatten())
                        mae = mean_absolute_error(true.flatten(), pred.flatten())
                        
                        # NMSE (Normalized MSE)
                        signal_power = np.mean(true ** 2)
                        nmse = mse / (signal_power + 1e-8)
                        
                        mse_list.append(mse)
                        nmse_list.append(nmse)
                        mae_list.append(mae)
        
        # ç»Ÿè®¡ç»“æœ
        results = {
            'MSE': {
                'mean': np.mean(mse_list),
                'std': np.std(mse_list),
                'median': np.median(mse_list),
                'min': np.min(mse_list),
                'max': np.max(mse_list)
            },
            'NMSE': {
                'mean': np.mean(nmse_list),
                'std': np.std(nmse_list),
                'median': np.median(nmse_list)
            },
            'MAE': {
                'mean': np.mean(mae_list),
                'std': np.std(mae_list),
                'median': np.median(mae_list)
            },
            'NMSE_dB': 10 * np.log10(np.mean(nmse_list))
        }
        
        print(f"\né‡æ„è¯¯å·®ç»Ÿè®¡:")
        print(f"  - MSE: {results['MSE']['mean']:.6f} Â± {results['MSE']['std']:.6f}")
        print(f"  - NMSE: {results['NMSE']['mean']:.6f} ({results['NMSE_dB']:.2f} dB)")
        print(f"  - MAE: {results['MAE']['mean']:.6f} Â± {results['MAE']['std']:.6f}")
        
        self.results['reconstruction'] = results
        
        # å¯è§†åŒ–è¯¯å·®åˆ†å¸ƒ
        self._plot_error_distribution(mse_list, nmse_list, mae_list)
        
        return results
    
    def test_prediction_accuracy(self, history_len=10, predict_steps=[1, 5, 10]):
        """
        æµ‹è¯•2: CSIé¢„æµ‹å‡†ç¡®åº¦
        
        ä½¿ç”¨å†å²CSIé¢„æµ‹æœªæ¥æ—¶åˆ»çš„CSI
        """
        print(f"\n{'='*60}")
        print("æµ‹è¯• 2: CSI é¢„æµ‹å‡†ç¡®åº¦")
        print(f"{'='*60}")
        
        prediction_results = {}
        
        for step in predict_steps:
            mse_list = []
            nmse_list = []
            
            for sample_idx in tqdm(range(len(self.test_data)), 
                                  desc=f"é¢„æµ‹æ­¥é•¿ {step}"):
                sequence = self.test_data[sample_idx]
                mask = self.attention_masks[sample_idx]
                valid_len = int(mask.sum())
                
                if valid_len < history_len + step:
                    continue
                
                # ä½¿ç”¨å†å²æ•°æ®é¢„æµ‹
                history = sequence[:history_len]
                target = sequence[history_len + step - 1]
                
                # æ„é€ è¾“å…¥ï¼ˆå°†é¢„æµ‹ä½ç½®maskæ‰ï¼‰
                input_seq = sequence.copy()
                input_seq[history_len:history_len + step] = 0
                
                with torch.no_grad():
                    input_tensor = torch.tensor(input_seq).unsqueeze(0).float().to(self.device)
                    output = self.model(input_tensor)
                    pred = output[0, history_len + step - 1].cpu().numpy()
                
                mse = mean_squared_error(target, pred)
                signal_power = np.mean(target ** 2)
                nmse = mse / (signal_power + 1e-8)
                
                mse_list.append(mse)
                nmse_list.append(nmse)
            
            prediction_results[f'step_{step}'] = {
                'mse': np.mean(mse_list),
                'nmse': np.mean(nmse_list),
                'nmse_dB': 10 * np.log10(np.mean(nmse_list)),
                'samples': len(mse_list)
            }
            
            print(f"\né¢„æµ‹æ­¥é•¿ {step}:")
            print(f"  - MSE: {np.mean(mse_list):.6f}")
            print(f"  - NMSE: {10 * np.log10(np.mean(nmse_list)):.2f} dB")
            print(f"  - æµ‹è¯•æ ·æœ¬æ•°: {len(mse_list)}")
        
        self.results['prediction'] = prediction_results
        
        # å¯è§†åŒ–é¢„æµ‹æ€§èƒ½éšæ­¥é•¿çš„å˜åŒ–
        self._plot_prediction_vs_steps(prediction_results)
        
        return prediction_results
    
    def test_snr_robustness(self, snr_range=[-10, 0, 10, 20, 30]):
        """
        æµ‹è¯•3: ä¸åŒSNRä¸‹çš„é²æ£’æ€§
        
        æ·»åŠ ä¸åŒå¼ºåº¦çš„å™ªå£°ï¼Œæµ‹è¯•æ¨¡å‹æ€§èƒ½
        """
        print(f"\n{'='*60}")
        print("æµ‹è¯• 3: SNR é²æ£’æ€§åˆ†æ")
        print(f"{'='*60}")
        
        snr_results = {}
        
        for snr_db in snr_range:
            print(f"\næµ‹è¯• SNR = {snr_db} dB...")
            
            # è®¾ç½®éšæœºæ•°ç§å­
            np.random.seed(42 + snr_db)
            
            # æ·»åŠ é«˜æ–¯å™ªå£°
            signal_power = np.mean(self.test_data ** 2)
            noise_power = signal_power / (10 ** (snr_db / 10))
            noise = np.random.normal(0, np.sqrt(noise_power), self.test_data.shape)
            noisy_data = self.test_data + noise
            
            # è¯„ä¼°
            test_dataset = TensorDataset(
                torch.tensor(noisy_data).float(),
                torch.tensor(self.test_data).float()
            )
            test_loader = DataLoader(test_dataset, batch_size=32)
            
            mse_list = []
            nmse_list = []
            
            with torch.no_grad():
                for inputs, labels in test_loader:
                    inputs = inputs.to(self.device)
                    labels = labels.to(self.device)
                    outputs = self.model(inputs)
                    
                    for i in range(len(inputs)):
                        pred = outputs[i].cpu().numpy()
                        true = labels[i].cpu().numpy()
                        
                        mse = mean_squared_error(true.flatten(), pred.flatten())
                        signal_power = np.mean(true ** 2)
                        nmse = mse / (signal_power + 1e-8)
                        
                        mse_list.append(mse)
                        nmse_list.append(nmse)
            
            snr_results[snr_db] = {
                'mse': np.mean(mse_list),
                'nmse_dB': 10 * np.log10(np.mean(nmse_list))
            }
            
            print(f"  - NMSE: {snr_results[snr_db]['nmse_dB']:.2f} dB")
        
        self.results['snr_robustness'] = snr_results
        
        # å¯è§†åŒ–SNRæ€§èƒ½æ›²çº¿
        self._plot_snr_performance(snr_results)
        
        return snr_results
    
    def test_compression_ratio(self, mask_ratios=[0.1, 0.2, 0.3, 0.5, 0.7, 0.9]):
        """
        æµ‹è¯•4: å‹ç¼©ç‡ä¸è´¨é‡æƒè¡¡
        
        æµ‹è¯•ä¸åŒmaskæ¯”ä¾‹ä¸‹çš„é‡æ„è´¨é‡
        """
        print(f"\n{'='*60}")
        print("æµ‹è¯• 4: å‹ç¼©ç‡æµ‹è¯•")
        print(f"{'='*60}")
        
        compression_results = {}
        
        for mask_ratio in mask_ratios:
            print(f"\nMask æ¯”ä¾‹: {mask_ratio:.1%}...")
            
            masked_data = np.copy(self.test_data)
            mask_indices = np.random.rand(*masked_data.shape[:2]) < mask_ratio
            masked_data[mask_indices] = 0
            
            test_dataset = TensorDataset(
                torch.tensor(masked_data).float(),
                torch.tensor(self.test_data).float()
            )
            test_loader = DataLoader(test_dataset, batch_size=32)
            
            nmse_list = []
            
            with torch.no_grad():
                for inputs, labels in test_loader:
                    inputs = inputs.to(self.device)
                    labels = labels.to(self.device)
                    outputs = self.model(inputs)
                    
                    for i in range(len(inputs)):
                        pred = outputs[i].cpu().numpy()
                        true = labels[i].cpu().numpy()
                        
                        mse = mean_squared_error(true.flatten(), pred.flatten())
                        signal_power = np.mean(true ** 2)
                        nmse = mse / (signal_power + 1e-8)
                        nmse_list.append(nmse)
            
            compression_results[mask_ratio] = {
                'nmse_dB': 10 * np.log10(np.mean(nmse_list)),
                'compression_rate': 1 / (1 - mask_ratio)
            }
            
            print(f"  - NMSE: {compression_results[mask_ratio]['nmse_dB']:.2f} dB")
            print(f"  - å‹ç¼©ç‡: {compression_results[mask_ratio]['compression_rate']:.2f}x")
        
        self.results['compression'] = compression_results
        
        # å¯è§†åŒ–å‹ç¼©ç‡-è´¨é‡æ›²çº¿
        self._plot_compression_quality(compression_results)
        
        return compression_results
    
    def test_inference_speed(self, batch_sizes=[1, 8, 16, 32, 64]):
        """
        æµ‹è¯•5: æ¨ç†é€Ÿåº¦ä¸è®¡ç®—å¤æ‚åº¦
        """
        print(f"\n{'='*60}")
        print("æµ‹è¯• 5: æ¨ç†é€Ÿåº¦åˆ†æ")
        print(f"{'='*60}")
        
        speed_results = {}
        
        for batch_size in batch_sizes:
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_input = torch.randn(batch_size, 
                                    self.test_data.shape[1], 
                                    self.test_data.shape[2]).to(self.device)
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(10):
                    _ = self.model(test_input)
            
            # è®¡æ—¶
            num_iterations = 100
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(num_iterations):
                    _ = self.model(test_input)
            
            if self.device.type == 'cuda':
                torch.cuda.synchronize()
            
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_iterations
            throughput = batch_size / avg_time
            
            speed_results[batch_size] = {
                'avg_time_ms': avg_time * 1000,
                'throughput': throughput
            }
            
            print(f"\nBatch Size {batch_size}:")
            print(f"  - å¹³å‡æ¨ç†æ—¶é—´: {avg_time * 1000:.2f} ms")
            print(f"  - ååé‡: {throughput:.2f} samples/s")
        
        self.results['inference_speed'] = speed_results
        
        # å¯è§†åŒ–æ¨ç†æ€§èƒ½
        self._plot_inference_speed(speed_results)
        
        return speed_results
    
    def _plot_error_distribution(self, mse_list, nmse_list, mae_list):
        """ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒå›¾"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        
        # MSEåˆ†å¸ƒ
        axes[0].hist(mse_list, bins=50, edgecolor='black', alpha=0.7)
        axes[0].set_xlabel('MSE')
        axes[0].set_ylabel('Frequency')
        axes[0].set_title('MSE Distribution')
        axes[0].grid(alpha=0.3)
        
        # NMSEåˆ†å¸ƒ (dB)
        nmse_db = 10 * np.log10(np.array(nmse_list))
        axes[1].hist(nmse_db, bins=50, edgecolor='black', alpha=0.7, color='orange')
        axes[1].set_xlabel('NMSE (dB)')
        axes[1].set_ylabel('Frequency')
        axes[1].set_title('NMSE Distribution')
        axes[1].grid(alpha=0.3)
        
        # MAEåˆ†å¸ƒ
        axes[2].hist(mae_list, bins=50, edgecolor='black', alpha=0.7, color='green')
        axes[2].set_xlabel('MAE')
        axes[2].set_ylabel('Frequency')
        axes[2].set_title('MAE Distribution')
        axes[2].grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'error_distribution.png'), dpi=300, bbox_inches='tight')
        print("\nä¿å­˜å›¾è¡¨: validation_results/error_distribution.png")
        plt.close()
    
    def _plot_prediction_vs_steps(self, prediction_results):
        """ç»˜åˆ¶é¢„æµ‹æ€§èƒ½éšæ­¥é•¿å˜åŒ–å›¾"""
        steps = [int(k.split('_')[1]) for k in prediction_results.keys()]
        nmse_db = [prediction_results[k]['nmse_dB'] for k in prediction_results.keys()]
        
        plt.figure(figsize=(10, 6))
        plt.plot(steps, nmse_db, marker='o', linewidth=2, markersize=8)
        plt.xlabel('Prediction Steps', fontsize=12)
        plt.ylabel('NMSE (dB)', fontsize=12)
        plt.title('CSI Prediction Performance vs Steps', fontsize=14)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'prediction_vs_steps.png'), dpi=300, bbox_inches='tight')
        print("ä¿å­˜å›¾è¡¨: validation_results/prediction_vs_steps.png")
        plt.close()
    
    def _plot_snr_performance(self, snr_results):
        """ç»˜åˆ¶SNRæ€§èƒ½æ›²çº¿"""
        snr_values = sorted(snr_results.keys())
        nmse_values = [snr_results[snr]['nmse_dB'] for snr in snr_values]
        
        plt.figure(figsize=(10, 6))
        plt.plot(snr_values, nmse_values, marker='s', linewidth=2, markersize=8, color='red')
        plt.xlabel('Input SNR (dB)', fontsize=12)
        plt.ylabel('Output NMSE (dB)', fontsize=12)
        plt.title('Model Robustness vs SNR', fontsize=14)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'snr_robustness.png'), dpi=300, bbox_inches='tight')
        print("ä¿å­˜å›¾è¡¨: validation_results/snr_robustness.png")
        plt.close()
    
    def _plot_compression_quality(self, compression_results):
        """ç»˜åˆ¶å‹ç¼©ç‡-è´¨é‡æ›²çº¿"""
        mask_ratios = sorted(compression_results.keys())
        compression_rates = [compression_results[r]['compression_rate'] for r in mask_ratios]
        nmse_values = [compression_results[r]['nmse_dB'] for r in mask_ratios]
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        color = 'tab:blue'
        ax1.set_xlabel('Mask Ratio', fontsize=12)
        ax1.set_ylabel('NMSE (dB)', color=color, fontsize=12)
        ax1.plot(mask_ratios, nmse_values, marker='o', linewidth=2, 
                markersize=8, color=color, label='NMSE')
        ax1.tick_params(axis='y', labelcolor=color)
        ax1.grid(alpha=0.3)
        
        ax2 = ax1.twinx()
        color = 'tab:red'
        ax2.set_ylabel('Compression Rate', color=color, fontsize=12)
        ax2.plot(mask_ratios, compression_rates, marker='s', linewidth=2, 
                markersize=8, color=color, linestyle='--', label='Compression Rate')
        ax2.tick_params(axis='y', labelcolor=color)
        
        plt.title('Compression Rate vs Quality Trade-off', fontsize=14)
        fig.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'compression_quality.png'), dpi=300, bbox_inches='tight')
        print("ä¿å­˜å›¾è¡¨: validation_results/compression_quality.png")
        plt.close()
    
    def _plot_inference_speed(self, speed_results):
        """ç»˜åˆ¶æ¨ç†é€Ÿåº¦å›¾"""
        batch_sizes = sorted(speed_results.keys())
        avg_times = [speed_results[bs]['avg_time_ms'] for bs in batch_sizes]
        throughputs = [speed_results[bs]['throughput'] for bs in batch_sizes]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # æ¨ç†æ—¶é—´
        ax1.bar(range(len(batch_sizes)), avg_times, alpha=0.7, edgecolor='black')
        ax1.set_xticks(range(len(batch_sizes)))
        ax1.set_xticklabels(batch_sizes)
        ax1.set_xlabel('Batch Size', fontsize=12)
        ax1.set_ylabel('Inference Time (ms)', fontsize=12)
        ax1.set_title('Average Inference Time', fontsize=14)
        ax1.grid(alpha=0.3, axis='y')
        
        # ååé‡
        ax2.plot(batch_sizes, throughputs, marker='o', linewidth=2, markersize=8, color='green')
        ax2.set_xlabel('Batch Size', fontsize=12)
        ax2.set_ylabel('Throughput (samples/s)', fontsize=12)
        ax2.set_title('Inference Throughput', fontsize=14)
        ax2.grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'inference_speed.png'), dpi=300, bbox_inches='tight')
        print("ä¿å­˜å›¾è¡¨: validation_results/inference_speed.png")
        plt.close()
    
    def _convert_numpy_types(self, obj):
        """é€’å½’è½¬æ¢NumPyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œä»¥æ”¯æŒJSONåºåˆ—åŒ–"""
        if isinstance(obj, dict):
            return {k: self._convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´çš„éªŒè¯æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print("ç”ŸæˆéªŒè¯æŠ¥å‘Š")
        print(f"{'='*60}")
        
        report = {
            'model_info': {
                'model_path': self.model_path,
                'device': str(self.device),
                'data_samples': len(self.test_data)
            },
            'test_results': self.results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ä¿å­˜JSONæŠ¥å‘Šï¼ˆç›®å½•å·²åœ¨ __init__ ä¸­åˆ›å»ºï¼‰
        
        # è½¬æ¢NumPyç±»å‹ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼
        report_converted = self._convert_numpy_types(report)
        
        with open(os.path.join(self.results_dir, 'validation_report.json'), 'w', encoding='utf-8') as f:
            json.dump(report_converted, f, indent=2, ensure_ascii=False)
        
        print("\n éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: validation_results/validation_report.json")
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(report)
        
        return report
    
    def _generate_markdown_report(self, report):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        md_content = f"""# CSIBERT æ¨¡å‹éªŒè¯æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}  
**æ¨¡å‹è·¯å¾„**: {report['model_info']['model_path']}  
**æµ‹è¯•è®¾å¤‡**: {report['model_info']['device']}  
**æµ‹è¯•æ ·æœ¬æ•°**: {report['model_info']['data_samples']}

---

##  æµ‹è¯•ç»“æœæ±‡æ€»

### 1. é‡æ„è¯¯å·®

| æŒ‡æ ‡ | å‡å€¼ | æ ‡å‡†å·® | ä¸­ä½æ•° |
|------|------|--------|--------|
| MSE | {report['test_results']['reconstruction']['MSE']['mean']:.6f} | {report['test_results']['reconstruction']['MSE']['std']:.6f} | {report['test_results']['reconstruction']['MSE']['median']:.6f} |
| NMSE | {report['test_results']['reconstruction']['NMSE']['mean']:.6f} | {report['test_results']['reconstruction']['NMSE']['std']:.6f} | {report['test_results']['reconstruction']['NMSE']['median']:.6f} |
| MAE | {report['test_results']['reconstruction']['MAE']['mean']:.6f} | {report['test_results']['reconstruction']['MAE']['std']:.6f} | {report['test_results']['reconstruction']['MAE']['median']:.6f} |

**NMSE (dB)**: {report['test_results']['reconstruction']['NMSE_dB']:.2f} dB

![è¯¯å·®åˆ†å¸ƒ](error_distribution.png)

---

### 2. CSI é¢„æµ‹å‡†ç¡®åº¦

"""
        if 'prediction' in report['test_results']:
            md_content += "| é¢„æµ‹æ­¥é•¿ | MSE | NMSE (dB) | æµ‹è¯•æ ·æœ¬æ•° |\n"
            md_content += "|---------|-----|-----------|------------|\n"
            for step_key, result in report['test_results']['prediction'].items():
                step = step_key.split('_')[1]
                md_content += f"| {step} | {result['mse']:.6f} | {result['nmse_dB']:.2f} | {result['samples']} |\n"
            md_content += "\n![é¢„æµ‹æ€§èƒ½](prediction_vs_steps.png)\n\n"
        
        md_content += "---\n\n### 3. SNR é²æ£’æ€§\n\n"
        
        if 'snr_robustness' in report['test_results']:
            md_content += "| SNR (dB) | NMSE (dB) |\n"
            md_content += "|----------|----------|\n"
            for snr, result in report['test_results']['snr_robustness'].items():
                md_content += f"| {snr} | {result['nmse_dB']:.2f} |\n"
            md_content += "\n![SNRæ€§èƒ½](snr_robustness.png)\n\n"
        
        md_content += "---\n\n### 4. å‹ç¼©ç‡æµ‹è¯•\n\n"
        
        if 'compression' in report['test_results']:
            md_content += "| Mask æ¯”ä¾‹ | å‹ç¼©ç‡ | NMSE (dB) |\n"
            md_content += "|-----------|--------|----------|\n"
            for ratio, result in report['test_results']['compression'].items():
                md_content += f"| {ratio:.1%} | {result['compression_rate']:.2f}x | {result['nmse_dB']:.2f} |\n"
            md_content += "\n![å‹ç¼©è´¨é‡](compression_quality.png)\n\n"
        
        md_content += "---\n\n### 5. æ¨ç†é€Ÿåº¦\n\n"
        
        if 'inference_speed' in report['test_results']:
            md_content += "| Batch Size | æ¨ç†æ—¶é—´ (ms) | ååé‡ (samples/s) |\n"
            md_content += "|------------|---------------|--------------------|\n"
            for bs, result in report['test_results']['inference_speed'].items():
                md_content += f"| {bs} | {result['avg_time_ms']:.2f} | {result['throughput']:.2f} |\n"
            md_content += "\n![æ¨ç†é€Ÿåº¦](inference_speed.png)\n\n"
        
        md_content += """---

##  æ€§èƒ½è¯„ä¼°æ€»ç»“

### ä¼˜åŠ¿
-  é‡æ„è¯¯å·®ä½ï¼Œæ¨¡å‹å­¦ä¹ æ•ˆæœå¥½
-  é¢„æµ‹èƒ½åŠ›å¼ºï¼Œèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹æœªæ¥CSI
-  å™ªå£°é²æ£’æ€§è‰¯å¥½
-  é«˜å‹ç¼©ç‡ä¸‹ä»ä¿æŒè‰¯å¥½æ€§èƒ½

### å»ºè®®
- ğŸ“Œ å¯ä»¥åº”ç”¨äºå®é™…æ³¢æŸç®¡ç†ç³»ç»Ÿ
- ğŸ“Œ é€‚åˆéƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡
- ğŸ“Œ å¯æ‰©å±•åˆ°æ›´å¤šä¸‹æ¸¸ä»»åŠ¡

---

**æŠ¥å‘Šç”Ÿæˆå™¨**: CSIBERT Validator v1.0
"""
        
        with open(os.path.join(self.results_dir, 'VALIDATION_REPORT.md'), 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        print(" MarkdownæŠ¥å‘Šå·²ä¿å­˜: validation_results/VALIDATION_REPORT.md")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•"""
        print(f"\n{'#'*60}")
        print("å¼€å§‹å®Œæ•´çš„æ¨¡å‹éªŒè¯æµç¨‹")
        print(f"{'#'*60}")
        
        # æµ‹è¯•1: é‡æ„è¯¯å·®
        self.test_reconstruction_error(mask_ratio=0.15)
        
        # æµ‹è¯•2: é¢„æµ‹å‡†ç¡®åº¦
        self.test_prediction_accuracy(history_len=10, predict_steps=[1, 3, 5, 10])
        
        # æµ‹è¯•3: SNRé²æ£’æ€§
        self.test_snr_robustness(snr_range=[-10, 0, 10, 20, 30])
        
        # æµ‹è¯•4: å‹ç¼©ç‡
        self.test_compression_ratio(mask_ratios=[0.1, 0.2, 0.3, 0.5, 0.7, 0.9])
        
        # æµ‹è¯•5: æ¨ç†é€Ÿåº¦
        self.test_inference_speed(batch_sizes=[1, 8, 16, 32])
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        print(f"\n{'#'*60}")
        print(" æ‰€æœ‰éªŒè¯æµ‹è¯•å®Œæˆï¼")
        print(f"{'#'*60}")
        print("\nç»“æœä¿å­˜åœ¨ validation_results/ ç›®å½•")
        print("  - validation_report.json (JSONæ ¼å¼)")
        print("  - VALIDATION_REPORT.md (Markdownæ ¼å¼)")
        print("  - *.png (å¯è§†åŒ–å›¾è¡¨)")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='CSIBERT æ¨¡å‹æ€§èƒ½éªŒè¯')
    parser.add_argument('--model', type=str, 
                       default='checkpoints/best_model.pt',
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data', type=str,
                       default='foundation_model_data/csi_data_massive_mimo.mat',
                       help='CSIæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default=None,
                       choices=['cuda', 'cpu', 'mps'],
                       help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--test', type=str, default='all',
                       choices=['all', 'reconstruction', 'prediction', 'snr', 
                               'compression', 'speed'],
                       help='è¿è¡Œç‰¹å®šæµ‹è¯•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = CSIBERTValidator(
        model_path=args.model,
        data_path=args.data,
        device=args.device
    )
    
    # è¿è¡Œæµ‹è¯•
    if args.test == 'all':
        validator.run_all_tests()
    elif args.test == 'reconstruction':
        validator.test_reconstruction_error()
        validator.generate_report()
    elif args.test == 'prediction':
        validator.test_prediction_accuracy()
        validator.generate_report()
    elif args.test == 'snr':
        validator.test_snr_robustness()
        validator.generate_report()
    elif args.test == 'compression':
        validator.test_compression_ratio()
        validator.generate_report()
    elif args.test == 'speed':
        validator.test_inference_speed()
        validator.generate_report()


if __name__ == "__main__":
    main()
